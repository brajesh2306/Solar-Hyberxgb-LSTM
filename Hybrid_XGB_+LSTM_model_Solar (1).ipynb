{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg6-4wPAYrK_",
        "outputId": "8aca2c48-e9d9-4b7d-8854-fcd495fcb8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.21)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost tensorflow pandas numpy scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/complete_solar_data2.csv')\n",
        "\n",
        "# Convert DATE_TIME to datetime format\n",
        "df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'], errors='coerce')\n",
        "\n",
        "# Extract time-based features\n",
        "\n",
        "df['MONTH'] = df['DATE_TIME'].dt.month\n",
        "df['DAY'] = df['DATE_TIME'].dt.day\n",
        "df['HOUR'] = df['DATE_TIME'].dt.hour\n",
        "\n",
        "# Drop DATE_TIME column\n",
        "df.drop(columns=['DATE_TIME'], inplace=True)\n",
        "\n",
        "# Drop NaN values (if any)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define target column\n",
        "target_column = 'AC_POWER'\n",
        "\n",
        "# Separate features and target\n",
        "features = df.drop(columns=[target_column])\n",
        "target = df[target_column]\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM (Sliding Window Technique)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build Improved LSTM Model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[2]))),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    LSTM(32, return_sequences=False),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='relu')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Performance Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate Accuracy using MAPE\n",
        "mape = np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100\n",
        "accuracy = 100 - mape  # Accuracy formula\n",
        "\n",
        "# Print Results\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R² Score: {r2}\")\n",
        "print(f\"Model Accuracy (100 - MAPE): {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GaAlGsmjXOZ",
        "outputId": "e2930bd2-a23f-47ff-a4d2-49020b24f574"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step - loss: 91516.6953 - val_loss: 17282.4883 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 23458.8496 - val_loss: 16965.6426 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - loss: 22358.5059 - val_loss: 14773.6660 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 21332.3555 - val_loss: 13878.6182 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 20009.2422 - val_loss: 14070.0205 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 19485.7188 - val_loss: 15289.6377 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m844/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19311.4863\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 19310.0234 - val_loss: 14806.9014 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 18554.4785 - val_loss: 13245.5586 - learning_rate: 5.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - loss: 18130.1621 - val_loss: 13053.9395 - learning_rate: 5.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 18032.0566 - val_loss: 13185.2725 - learning_rate: 5.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 17619.2148 - val_loss: 13031.4160 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16990.9414 - val_loss: 13043.9346 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 17089.5176 - val_loss: 12934.5303 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 16633.5469 - val_loss: 13123.7334 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16896.9277 - val_loss: 12737.0146 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 17076.7871 - val_loss: 13417.6973 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 17022.5312 - val_loss: 12912.3555 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 16598.0684 - val_loss: 12508.5498 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16358.9893 - val_loss: 12584.1162 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 16583.4355 - val_loss: 12607.1582 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m846/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 16205.9814\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 16205.8242 - val_loss: 12977.1309 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 16128.0703 - val_loss: 12457.0205 - learning_rate: 2.5000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - loss: 16112.7354 - val_loss: 12219.1094 - learning_rate: 2.5000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 16170.6875 - val_loss: 12227.2588 - learning_rate: 2.5000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - loss: 14633.6904 - val_loss: 12503.2344 - learning_rate: 2.5000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 15743.3574 - val_loss: 12196.0625 - learning_rate: 2.5000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 15014.8262 - val_loss: 12339.1494 - learning_rate: 2.5000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 15171.7480 - val_loss: 12330.0947 - learning_rate: 2.5000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 15685.3936 - val_loss: 12033.3223 - learning_rate: 2.5000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - loss: 15423.7920 - val_loss: 12585.1650 - learning_rate: 2.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 14972.7412 - val_loss: 11969.4297 - learning_rate: 2.5000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 14850.3691 - val_loss: 12283.5742 - learning_rate: 2.5000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 15478.4502 - val_loss: 12138.3623 - learning_rate: 2.5000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m845/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15263.5293\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 15262.9639 - val_loss: 12556.3652 - learning_rate: 2.5000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 14874.2314 - val_loss: 12088.6074 - learning_rate: 1.2500e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 14114.0479 - val_loss: 12004.7881 - learning_rate: 1.2500e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m845/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14713.8154\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - loss: 14713.8701 - val_loss: 11994.2236 - learning_rate: 1.2500e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m847/847\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 15311.1475 - val_loss: 11971.2764 - learning_rate: 6.2500e-05\n",
            "Epoch 38: early stopping\n",
            "Restoring model weights from the end of the best epoch: 31.\n",
            "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Mean Absolute Error (MAE): 37.69765660266326\n",
            "Mean Squared Error (MSE): 11969.429345657678\n",
            "Root Mean Squared Error (RMSE): 109.40488721102763\n",
            "R² Score: 0.9081633995006861\n",
            "Model Accuracy (100 - MAPE): -inf%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to take user input and make a prediction\n",
        "def predict_ac_power():\n",
        "    print(\"\\nEnter feature values for prediction:\")\n",
        "\n",
        "    # Accept user input for all features\n",
        "    feature_values = []\n",
        "    for col in features.columns:\n",
        "        val = float(input(f\"Enter {col}: \"))\n",
        "        feature_values.append(val)\n",
        "\n",
        "    # Convert input to numpy array\n",
        "    feature_array = np.array(feature_values).reshape(1, -1)\n",
        "\n",
        "    # Scale input using the previously fitted scaler\n",
        "    feature_array_scaled = scaler.transform(feature_array)\n",
        "\n",
        "    # Reshape for LSTM input\n",
        "    feature_array_reshaped = np.reshape(feature_array_scaled, (1, 1, feature_array_scaled.shape[1]))\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_power = model.predict(feature_array_reshaped)\n",
        "\n",
        "    # Print the prediction\n",
        "    print(f\"\\nPredicted AC Power: {predicted_power[0][0]:.2f}\")\n",
        "\n",
        "# Call function for user input and prediction\n",
        "predict_ac_power()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tY7xWz2qwZW",
        "outputId": "426aa6e3-d108-4d3c-ebf9-4d9893427be5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter feature values for prediction:\n",
            "Enter AMBIENT_TEMPERATURE: 29.91\n",
            "Enter MODULE_TEMPERATURE: 45.68\n",
            "Enter IRRADIATION: 0.71\n",
            "Enter DAILY_YIELD: 2176.6\n",
            "Enter MONTH: 5\n",
            "Enter DAY: 29\n",
            "Enter HOUR: 10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "Predicted AC Power: 908.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "-3EKDEzJLakp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"solar_energy_model.h5\")  # Save in HDF5 format\n",
        "\n",
        "# Save the scaler\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print(\"Model and scaler saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUQiO3OvLcuE",
        "outputId": "b1aa8a5b-44b6-4f10-dc8a-e6225c78d273"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and scaler saved successfully!\n"
          ]
        }
      ]
    }
  ]
}